{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ffde3c3e-fd2b-4bbf-9075-d3741119a13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import nltk\n",
    "from tqdm.notebook import tqdm\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "from nltk.lm import MLE\n",
    "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d970865c-2c0b-4e33-ae5d-e3cea70ccb49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/calvang/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ebaed11-239f-439f-9849-611f797b8598",
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_bleu_score(ref, test):\n",
    "    \"\"\"Find self-bleus between two corpuses, return avg and list\"\"\"\n",
    "    scores = []\n",
    "    for sent in test:\n",
    "        score = sentence_bleu(ref, sent)\n",
    "        scores.append(score)\n",
    "    avg_score = sum(scores)/len(scores)\n",
    "    return avg_score, scores\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d22b91dd-2b70-450b-93cd-dddc14b6a22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_perplexity(ref):\n",
    "    n = 1\n",
    "    train_data, padded_vocab = padded_everygram_pipeline(n, ref)\n",
    "    model = MLE(n)\n",
    "    model.fit(train_data, padded_vocab)\n",
    "    return model\n",
    "\n",
    "def perplexity_score(model, ref, test):\n",
    "    # unigram perplexity\n",
    "    n = 1\n",
    "    test_data, _ = padded_everygram_pipeline(n, test)\n",
    "    \n",
    "    scores = []\n",
    "    for i, sent in tqdm(enumerate(test_data)):\n",
    "        score = model.perplexity(sent)\n",
    "        scores.append(score)\n",
    "        \n",
    "    avg_score = sum(scores)/len(scores)\n",
    "    return avg_score, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "65a9a4fd-535c-4a83-97ef-2bed14c10b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_gan(ref, test, ppl_model):\n",
    "#     bleu = corpus_bleu(ref, test)\n",
    "    \n",
    "#     print(\"Corpus BLEU score:\", bleu)\n",
    "    \n",
    "#     self_bleu, self_bleu_scores = self_bleu_score(ref, test)\n",
    "    \n",
    "#     print(\"self-BLEU score:\", self_bleu)\n",
    "    \n",
    "    ppl, ppl_scores = perplexity_score(ppl_model, ref, test)\n",
    "    \n",
    "    print(\"Average Unigram Perplexity score:\", ppl)\n",
    "    \n",
    "    results = pd.DataFrame({\n",
    "        \"real\": real,  \n",
    "        \"fake\": fake,\n",
    "        # \"self-bleu\": self_bleu_scores,\n",
    "        \"perplexity\": ppl_scores })\n",
    "    results.to_csv(f\"Eval/GAN_out_{datetime.now().strftime('%d-%m-%Y_%H-%M-%S')}.csv\")\n",
    "    return bleu, self_bleu, ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af958fc7-69c1-46c4-89eb-f96ee5e319cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"test_10k_filtered.csv\")\n",
    "real = test_df[\"text\"].values.tolist()\n",
    "ref = [list(map(str.lower, nltk.tokenize.word_tokenize(sent.strip()))) \n",
    "                for sent in real]\n",
    "\n",
    "ppl_model = train_perplexity(ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9e83a0-ed28-45fe-81d5-fdec1748948c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus BLEU score: 4.074583648336054e-156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/calvang/Documents/College/EECS595/project/env/lib/python3.8/site-packages/nltk/translate/bleu_score.py:515: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "in_file = \"GAN_out_17-12-2021_20-03-47.csv\"\n",
    "\n",
    "out_df = pd.read_csv(in_file)\n",
    "fake = out_df[\"fake\"].values.tolist()\n",
    "test = [list(map(str.lower, nltk.tokenize.word_tokenize(sent.strip()))) \n",
    "            for sent in fake]\n",
    "\n",
    "scores = test_gan(ref, test, ppl_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "81875d12-4aa4-4ab3-8ad4-1b60c23ff872",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19bea35c1cc148a5b1e8926969804784",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Unigram Perplexity score: 858.6634543817936\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'bleu' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_42706/4259369387.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mppl_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_perplexity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_gan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mppl_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_42706/2246073269.py\u001b[0m in \u001b[0;36mtest_gan\u001b[0;34m(ref, test, ppl_model)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \"perplexity\": ppl_scores })\n\u001b[1;32m     19\u001b[0m     \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Eval/GAN_out_{datetime.now().strftime('%d-%m-%Y_%H-%M-%S')}.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mbleu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself_bleu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mppl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'bleu' is not defined"
     ]
    }
   ],
   "source": [
    "in_file = \"GAN_out_17-12-2021_20-03-47.csv\"\n",
    "\n",
    "out_df = pd.read_csv(in_file)\n",
    "fake = out_df[\"fake\"].values.tolist()\n",
    "test = [list(map(str.lower, nltk.tokenize.word_tokenize(sent.strip()))) \n",
    "            for sent in fake]\n",
    "ppl_model = train_perplexity(test)\n",
    "\n",
    "scores = test_gan(ref, test, ppl_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "39826d23-0c1e-43e9-8136-bedb3b3ddc1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05e2652faed34ded99bc32a41b352b29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Unigram Perplexity score: 1157.1086231987908\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'bleu' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_42706/171400444.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mppl_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_perplexity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_gan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mppl_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_42706/2246073269.py\u001b[0m in \u001b[0;36mtest_gan\u001b[0;34m(ref, test, ppl_model)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \"perplexity\": ppl_scores })\n\u001b[1;32m     19\u001b[0m     \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Eval/GAN_out_{datetime.now().strftime('%d-%m-%Y_%H-%M-%S')}.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mbleu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself_bleu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mppl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'bleu' is not defined"
     ]
    }
   ],
   "source": [
    "in_file = \"GPT_out_19-12-2021_19-46-31.csv\"\n",
    "\n",
    "out_df = pd.read_csv(in_file)\n",
    "fake = out_df[\"fake\"].values.tolist()\n",
    "test = [list(map(str.lower, nltk.tokenize.word_tokenize(sent.strip()))) \n",
    "            for sent in fake]\n",
    "ppl_model = train_perplexity(test)\n",
    "\n",
    "scores = test_gan(ref, test, ppl_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer_env",
   "language": "python",
   "name": "transformer_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
